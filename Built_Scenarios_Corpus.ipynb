{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import time\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.data import load\n",
    "from string import punctuation\n",
    "from nltk.stem import SnowballStemmer\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob as tb \n",
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "spanish_stopwords=stopwords.words('spanish')\n",
    "spanish_verbs=stopwords.words('spanish_verbs')\n",
    "non_words=list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dataset\n",
    "dataset=pd.read_csv('/home/user/Documents/Preguntas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función, elimina faltas ortográficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VerificarLexico(word):\n",
    "    url=\"https://es.wiktionary.org/wiki/\"+str(word)\n",
    "    source_code=requests.get(url)\n",
    "    html_text=source_code.text\n",
    "    soup=BeautifulSoup(html_text,\"html.parser\")\n",
    "    meanin=soup.find('div',{'class':'toctitle'})\n",
    "    if not meanin==None:\n",
    "        return(word)\n",
    "    else: return(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir Corpus en 4 escenarios: CLCV, CLSV, SLCV, SLSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CON LEMATIZACIÓN RESPETANDO LOS VERBOS\n",
    "document1CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P1'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document1CLCV:\n",
    "        document1CLCV.append(testi)\n",
    "        \n",
    "#CON LEMATIZACIÓN SIN LOS VERBOS\n",
    "document1CLSV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P1'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(spanish_verbs)]\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document10CLSV:\n",
    "        document10CLSV.append(testi)\n",
    "\n",
    "#SIN LEMATIZACIÓN RESPETANDO LOS VERBOS        \n",
    "document1SLCV = []\n",
    "for i in range(0,len(data)):\n",
    "    testi=(dataset['P1'][i])\n",
    "    testi= testi.lower().split()\n",
    "    testi=[word for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document1SLCV:\n",
    "        document1SLCV.append(testi)\n",
    "\n",
    "#SIN LEMATIZACIÓN SIN LOS VERBOS\n",
    "document1SLSV = []\n",
    "for i in range(0,214):\n",
    "    testi=(dataset['P1'][i])\n",
    "    testi= testi.lower().split()\n",
    "    testi=[word for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[word for word in testi if not word in set(spanish_verbs)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document1SLSV:\n",
    "        document1SLSV.append(testi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir Corpus en 4 escenarios: CLCV, CLSV, SLCV, SLSV sin faltas ortográficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CON LEMATIZACIÓN RESPETANDO LOS VERBOS\n",
    "document1CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P1'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[VerificarLexico(word) for word in testi]#ELIMINAR PALABRAS QUE NO EXISTEN EN ESPAÑOL\n",
    "    sleep(20)#olvida ip pausa 20 sec\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document1CLCV:\n",
    "        document1CLCV.append(testi)\n",
    "sleep(700)\n",
    "\n",
    "#CON LEMATIZACIÓN SIN LOS VERBOS\n",
    "document1CLSV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P1'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(spanish_verbs)]\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[VerificarLexico(word) for word in testi]#ELIMINAR PALABRAS QUE NO EXISTEN EN ESPAÑOL\n",
    "    sleep(20)#olvida ip pausa 20 sec\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document10CLSV:\n",
    "        document10CLSV.append(testi)\n",
    "sleep(700)\n",
    "\n",
    "#SIN LEMATIZACIÓN RESPETANDO LOS VERBOS        \n",
    "document1SLCV = []\n",
    "for i in range(0,len(data)):\n",
    "    testi=(dataset['P1'][i])\n",
    "    testi= testi.lower().split()\n",
    "    testi=[word for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[VerificarLexico(word) for word in testi]#ELIMINAR PALABRAS QUE NO EXISTEN EN ESPAÑOL\n",
    "    sleep(20)#olvida ip pausa 20 sec\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document1SLCV:\n",
    "        document1SLCV.append(testi)\n",
    "sleep(700)\n",
    "        \n",
    "#SIN LEMATIZACIÓN SIN LOS VERBOS\n",
    "document1SLSV = []\n",
    "for i in range(0,len(data)):\n",
    "    testi=(dataset['P1'][i])\n",
    "    testi= testi.lower().split()\n",
    "    testi=[word for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[word for word in testi if not word in set(spanish_verbs)]\n",
    "    testi=[VerificarLexico(word) for word in testi]#ELIMINAR PALABRAS QUE NO EXISTEN EN ESPAÑOL\n",
    "    sleep(20)#olvida ip pausa 20 sec\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document1SLSV:\n",
    "        document1SLSV.append(testi)\n",
    "sleep(700)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
