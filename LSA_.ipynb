{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import time\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.data import load\n",
    "from string import punctuation\n",
    "from textblob import TextBlob as tb \n",
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from heapq import merge \n",
    "from timeit import default_timer as timer\n",
    "\n",
    "spanish_stopwords=stopwords.words('spanish')\n",
    "spanish_verbs=stopwords.words('spanish_verbs')\n",
    "non_words=list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dataset\n",
    "dataset=pd.read_csv('/home/user/Documents/Preguntas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir Corpus CLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CON LEMATIZACIÃ“N RESPETANDO LOS VERBOS\n",
    "document1CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P1'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document1CLCV:\n",
    "        document1CLCV.append(testi)\n",
    "\n",
    "document2CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P2'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document2CLCV:\n",
    "        document2CLCV.append(testi)\n",
    "\n",
    "document3CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P3'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document3CLCV:\n",
    "        document3CLCV.append(testi)\n",
    "        \n",
    "document4CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P4'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document4CLCV:\n",
    "        document4CLCV.append(testi)\n",
    "        \n",
    "document5CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P5'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document5CLCV:\n",
    "        document5CLCV.append(testi)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando TF-IDF para la SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title:Scikit-learn: Machine Learning in {P}ython\n",
    "*    Author: Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n",
    "         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n",
    "         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n",
    "            Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.\n",
    "*    Date: 2011\n",
    "*    Code version: 12.0\n",
    "*    Availability: http://scikit-learn.org/stable/about.html#citing-scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir la matriz SVD para cada pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=timer()\n",
    "Z=vectorizer.fit_transform(document1SLCV)\n",
    "lsa = TruncatedSVD(n_components=1,n_iter=1000)\n",
    "lsa.fit(Z)\n",
    "terms=vectorizer.get_feature_names()\n",
    "\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp=zip(terms,comp)\n",
    "    sortedterms=sorted(termsInComp,key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"Concept %d:\" %i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "        \n",
    "#################################################333        \n",
    "X=vectorizer.fit_transform(document2SLCV)\n",
    "\n",
    "\n",
    "lsa = TruncatedSVD(n_components=1,n_iter=1000)\n",
    "lsa.fit(X)\n",
    "terms=vectorizer.get_feature_names()\n",
    "\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp=zip(terms,comp)\n",
    "    sortedterms=sorted(termsInComp,key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"Concept %d:\" %i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "    print(\" \")\n",
    "\n",
    "    \n",
    "##**********************************************\n",
    "Y=vectorizer.fit_transform(document3SLCV)\n",
    "lsa = TruncatedSVD(n_components=1,n_iter=1000)\n",
    "lsa.fit(Y)\n",
    "terms=vectorizer.get_feature_names()\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp=zip(terms,comp)\n",
    "    sortedterms=sorted(termsInComp,key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"Concept %d:\" %i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "    print(\" \")\n",
    "\n",
    "#############################################\n",
    "Q=vectorizer.fit_transform(document4SLCV)\n",
    "lsa = TruncatedSVD(n_components=1,n_iter=1000)\n",
    "lsa.fit(Q)\n",
    "terms=vectorizer.get_feature_names()\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp=zip(terms,comp)\n",
    "    sortedterms=sorted(termsInComp,key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"Concept %d:\" %i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])\n",
    "\n",
    "###################################################\n",
    "W=vectorizer.fit_transform(document5SLCV)\n",
    "lsa = TruncatedSVD(n_components=1,n_iter=1000)\n",
    "lsa.fit(W)\n",
    "terms=vectorizer.get_feature_names()\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    termsInComp=zip(terms,comp)\n",
    "    sortedterms=sorted(termsInComp,key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"Concept %d:\" %i)\n",
    "    for term in sortedterms:\n",
    "        print(term[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
