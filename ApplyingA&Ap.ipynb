{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title: tfidf.py\n",
    "source code\n",
    "*    Author: Loria, S\n",
    "*    Date: 2013\n",
    "*    Code version: 1.0\n",
    "*    Availability: https://gist.github.com/sloria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, blob):\n",
    "    return blob.words.count(word)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para Proceso \"A\"\n",
    "Juntar los \"m\" documentos por tema (al final N documentos en 4 escenarios)\n",
    "\n",
    "\\begin{equation}\n",
    "     L_k=\\bigcup_{i=1}^{m}X_i ,\\forall k, 1\\rightarrow m\n",
    "     \\label{eq:lk}\n",
    "\\end{equation}\n",
    "Donde $X_i$ es la lista de tokens en cada documento $m_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONJUNTO DE SLCV\n",
    "\n",
    "X5=  ' '.join(corpus5SLCV)\n",
    "X5 = X5.lower().split()\n",
    "L5=Xb(' '.join(X5))\n",
    "\n",
    "X4=  ' '.join(corpus4SLCV)\n",
    "X4 = X4.lower().split()\n",
    "L4=Xb(' '.join(X4))\n",
    "\n",
    "X3=  ' '.join(corpus3SLCV)\n",
    "X3 = X3.lower().split()\n",
    "L3=Xb(' '.join(X3))\n",
    "\n",
    "X2=  ' '.join(corpus2SLCV)\n",
    "X2 = X2.lower().split()\n",
    "L2=Xb(' '.join(X2))\n",
    "\n",
    "X1=  ' '.join(corpus1SLCV)\n",
    "X1 = X1.lower().split()\n",
    "L1=Xb(' '.join(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"A\" Hallando TFIDF en los 10 documentos \n",
    "\n",
    "\\begin{equation}\n",
    "   A_k = max(tf-idf(L_k))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=timer()\n",
    "bloblist = [L1,L2,L3,L4,L5]\n",
    "listaA=[]\n",
    "for i, tags in enumerate(bloblist):\n",
    "    scores = {words: tfidf(words, tags, bloblist) for words in tags.words}\n",
    "    sorted_words= sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for words, score in sorted_words[:10]:\n",
    "        words= words.lower().split()\n",
    "        tfidf_words=' '.join(words)\n",
    "        listaA.append(tfidf_words)\n",
    "end=timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para Proceso \"A' \"\n",
    "Hallar la palabra con TF-IDF más alto en cada documento\n",
    "\n",
    "\\begin{equation}\n",
    "    L_ki=\\bigcup_{i=1}^{m}max(tf-idf(X_i)) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "#########################################################3\n",
    "ss=list([ ''.join(grams) for grams in document5SLCV])\n",
    "ju=(' '.join(ss))\n",
    "me=set([ju])\n",
    "x={}\n",
    "counter=0\n",
    "for i in range(0,len(document5SLCV)):\n",
    "    testi= document5SLCV[i]\n",
    "    x[testi.split(\":\")[0]]= counter\n",
    "    counter += 1\n",
    "tfidf = TfidfVectorizer(vocabulary = me, ngram_range=(1,1))\n",
    "tfs = tfidf.fit_transform(x.keys())\n",
    "Lk5 = tfidf.get_Lk()\n",
    "document_index = [n for n in x]\n",
    "rows, cols = tfs.nonzero()\n",
    "for row, col in zip(rows, cols):\n",
    "    print((Lk5[col], tfs[row, col]))\n",
    "\n",
    "################################################# \n",
    "ss=list([ ''.join(grams) for grams in document4SLCV])\n",
    "ju=(' '.join(ss))\n",
    "me=set([ju])\n",
    "x={}\n",
    "counter=0\n",
    "for i in range(0,len(document4SLSLCV)):\n",
    "    testi= document4SLCV[i]\n",
    "    x[testi.split(\":\")[0]]= counter\n",
    "    counter += 1\n",
    "tfidf = TfidfVectorizer(vocabulary = me, ngram_range=(1,1))\n",
    "tfs = tfidf.fit_transform(x.keys())\n",
    "Lk4 = tfidf.get_Lk()\n",
    "document_index = [n for n in x]\n",
    "rows, cols = tfs.nonzero()\n",
    "for row, col in zip(rows, cols):\n",
    "    print((Lk4[col], tfs[row, col]))\n",
    "\n",
    "################################################# \n",
    "ss=list([ ''.join(grams) for grams in document3SLCV])\n",
    "ju=(' '.join(ss))\n",
    "me=set([ju])\n",
    "x={}\n",
    "counter=0\n",
    "for i in range(0,len(document3SLCV)):\n",
    "    testi= document3SLCV[i]\n",
    "    x[testi.split(\":\")[0]]= counter\n",
    "    counter += 1\n",
    "tfidf = TfidfVectorizer(vocabulary = me, ngram_range=(1,1))\n",
    "tfs = tfidf.fit_transform(x.keys())\n",
    "Lk3 = tfidf.get_Lk()\n",
    "document_index = [n for n in x]\n",
    "rows, cols = tfs.nonzero()\n",
    "for row, col in zip(rows, cols):\n",
    "    print((Lk3[col], tfs[row, col]))\n",
    "\n",
    "################################################# \n",
    "ss=list([ ''.join(grams) for grams in document2SLCV])\n",
    "ju=(' '.join(ss))\n",
    "me=set([ju])\n",
    "x={}\n",
    "counter=0\n",
    "for i in range(0,len(document2SLCV)):\n",
    "    testi= document2SLCV[i]\n",
    "    x[testi.split(\":\")[0]]= counter\n",
    "    counter += 1\n",
    "tfidf = TfidfVectorizer(vocabulary = me, ngram_range=(1,1))\n",
    "tfs = tfidf.fit_transform(x.keys())\n",
    "Lk2 = tfidf.get_Lk()\n",
    "document_index = [n for n in x]\n",
    "rows, cols = tfs.nonzero()\n",
    "for row, col in zip(rows, cols):\n",
    "    print((Lk2[col], tfs[row, col]))\n",
    "\n",
    "################################################# \n",
    "ss=list([ ''.join(grams) for grams in document1SLCV])\n",
    "ju=(' '.join(ss))\n",
    "me=set([ju])\n",
    "x={}\n",
    "counter=0\n",
    "for i in range(0,len(document1SLCV)):\n",
    "    testi= document1SLCV[i]\n",
    "    x[testi.split(\":\")[0]]= counter\n",
    "    counter += 1\n",
    "tfidf = TfidfVectorizer(vocabulary = me, ngram_range=(1,1))\n",
    "tfs = tfidf.fit_transform(x.keys())\n",
    "Lk1 = tfidf.get_Lk()\n",
    "document_index = [n for n in x]\n",
    "rows, cols = tfs.nonzero()\n",
    "for row, col in zip(rows, cols):\n",
    "    print((Lk1[col], tfs[row, col]))\n",
    "################################################# \n",
    "\n",
    "end=timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juntar las palabras de TFIDF más alto en 1 sola lista por pregunta\n",
    "\\begin{equation}\n",
    "     L_ki'=\\bigcup_{i=1}^{m}L_ki,\\forall i,1\\rightarrow m \\wedge k,1\\rightarrow N\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lkp1=tb(' '.join(Lk1))\n",
    "Lkp2=tb(' '.join(Lk2))\n",
    "Lkp3=tb(' '.join(Lk3))\n",
    "Lkp4=tb(' '.join(Lk4))\n",
    "Lkp5=tb(' '.join(Lk5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"A' \" Hallando TFIDF en los 10 documentos \n",
    "\\begin{equation}\n",
    "    A'=max(tf-idf(L_ki'))\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=timer()\n",
    "bloblist = [Lkp1,Lkp2,Lkp3,Lkp4,Lkp5]\n",
    "listaA_=[]\n",
    "for i, tags in enumerate(bloblist):\n",
    "    scores = {words: tfidf(words, tags, bloblist) for words in tags.words}\n",
    "    sorted_words= sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for words, score in sorted_words[:10]:\n",
    "        words= words.lower().split()\n",
    "        tfidf_words=' '.join(words)\n",
    "        listaA_.append(tfidf_words)\n",
    "end=timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraer los documentos con las palabras en A y A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocRelevantes= []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P5'][i])\n",
    "    testi= testi.lower().split()\n",
    "    w=list(merge(testi, listaA_[50:54]))#CUATRO PALABRAS DE LA LISTA CONSTRUIDA POR A'\n",
    "    q=set(w)\n",
    "    if len(q)==len(testi):\n",
    "        DocRelevantes.append(testi)\n",
    "        ss=list(' '.join(grams) for grams in DocRelevantes)\n",
    "\n",
    "f = open('/home/admin/Documents/RESULTS/Preg5_4.txt','w')\n",
    "f.write((str(ss)))\n",
    "f.close()\n",
    "print(len(DocRelevantes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
