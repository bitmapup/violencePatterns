{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import time\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.data import load\n",
    "from string import punctuation\n",
    "from nltk.stem import SnowballStemmer\n",
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from heapq import merge \n",
    "from timeit import default_timer as timer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "spanish_stopwords=stopwords.words('spanish')\n",
    "spanish_verbs=stopwords.words('spanish_verbs')\n",
    "non_words=list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dataset\n",
    "dataset=pd.read_csv('/home/user/Documents/Preguntas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir Corpus CLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CON LEMATIZACIÓN RESPETANDO LOS VERBOS\n",
    "document1CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P1'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document1CLCV:\n",
    "        document1CLCV.append(testi)\n",
    "\n",
    "document2CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P2'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document2CLCV:\n",
    "        document2CLCV.append(testi)\n",
    "\n",
    "document3CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P3'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document3CLCV:\n",
    "        document3CLCV.append(testi)\n",
    "        \n",
    "document4CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P4'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document4CLCV:\n",
    "        document4CLCV.append(testi)\n",
    "        \n",
    "document5CLCV = []\n",
    "for i in range(0,len(dataset)):\n",
    "    testi=(dataset['P5'][i])\n",
    "    testi= testi.lower().split()\n",
    "    ps=SnowballStemmer('spanish')\n",
    "    testi=[word for word in testi if not word in set(non_words)]\n",
    "    testi=[ps.stem(word) for word in testi if not word in set(spanish_stopwords)]\n",
    "    testi=(' '.join(testi))\n",
    "    if testi not in document5CLCV:\n",
    "        document5CLCV.append(testi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir diccionarios del corpus, utilizar Gensim para LDA\n",
    "En la pregunta 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title:Understanding LDA implementation using gensim\n",
    "*    Author: James Mishra\n",
    "*    Date: 2017\n",
    "*    Code version: 12.0\n",
    "*    Availability: https://stackoverflow.com/questions/20349958/understanding-lda-implementation-using-gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in document1CLSV:# EN pregunta 1\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # add tokens to list\n",
    "    texts.append(tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imprimir las 10 palabras top en 1 topic, pregunta 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.026*\"si\" + 0.015*\"mujer\" + 0.014*\"sexual\" + 0.013*\"mujeres\" + 0.013*\"personas\" + 0.010*\"orientación\" + 0.010*\"solo\" + 0.009*\"mas\" + 0.009*\"compañeros\" + 0.008*\"docentes\"')]\n",
      "0.0005392728327109353\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "start=timer()\n",
    "print(ldamodel.print_topics(num_topics=1, num_words=10))\n",
    "end=timer()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
